<!DOCTYPE html><HTML lang="en"><head><script charset="utf-8" src="../../assets/default/multidoc_injector.js" type="text/javascript"></script><script charset="utf-8" src="../../assets/default/flexsearch.bundle.js" type="text/javascript"></script><script charset="utf-8" src="../../assets/default/flexsearch_integration.js" type="text/javascript"></script><meta charset="UTF-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Home · MCMCDiagnosticTools.jl</title><script data-outdated-warner="" src="assets/warner.js"></script><link href="https://turinglang.github.io/MCMCDiagnosticTools.jl/" rel="canonical"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script data-main="assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" data-theme-name="documenter-dark" data-theme-primary-dark="" href="assets/themes/documenter-dark.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" data-theme-name="documenter-light" data-theme-primary="" href="assets/themes/documenter-light.css" rel="stylesheet" type="text/css"/><script src="assets/themeswap.js"></script><link href="../../assets/default/multidoc.css" rel="stylesheet" type="text/css"/><link href="../../assets/default/flexsearch.css" rel="stylesheet" type="text/css"/><link href="../../assets/multidoc-custom.css" rel="stylesheet" type="text/css"/></head><body><nav id="multi-page-nav"><div class="hidden-on-mobile" id="nav-items"><div class="nav-dropdown"><button class="nav-item dropdown-label">Modelling languages</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../DynamicPPL/">DynamicPPL</a><a class="nav-link nav-item" href="../../JuliaBUGS/">JuliaBUGS</a><a class="nav-link nav-item" href="../../TuringGLM/">TuringGLM</a></ul></div><div class="nav-dropdown"><button class="nav-item dropdown-label">MCMC</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../AdvancedHMC/">AdvancedHMC</a><a class="nav-link nav-item" href="../../AbstractMCMC/">AbstractMCMC</a><a class="nav-link nav-item" href="../../ThermodynamicIntegration/">ThermodynamicIntegration</a><a class="nav-link nav-item" href="../../AdvancedPS/">AdvancedPS</a><a class="nav-link nav-item" href="../../EllipticalSliceSampling/">EllipticalSliceSampling</a><a class="nav-link nav-item" href="../../NestedSamplers/">NestedSamplers</a></ul></div><div class="nav-dropdown"><button class="nav-item dropdown-label">Diagnostics</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../MCMCChains/">MCMCChains</a><a class="nav-link active nav-item" href="../">MCMCDiagnosticTools</a><a class="nav-link nav-item" href="../../ParetoSmooth/">ParetoSmooth</a></ul></div><a class="nav-link nav-item" href="../../Bijectors/">Bijectors</a><a class="nav-link nav-item" href="../../TuringCallbacks/">TuringCallbacks</a><a class="nav-link nav-item" href="../../TuringBenchmarking/">TuringBenchmarking</a><div class="nav-dropdown"><button class="nav-item dropdown-label">Gaussian Processes</button><ul class="nav-dropdown-container"><a class="nav-link nav-item" href="../../AbstractGPs/">AbstractGPs</a><a class="nav-link nav-item" href="../../KernelFunctions/">KernelFunctions</a><a class="nav-link nav-item" href="../../ApproximateGPs/">ApproximateGPs</a></ul></div><div class="search nav-item"><input id="search-input" placeholder="Search..."/><ul class="suggestions hidden" id="search-result-container"></ul><div class="search-keybinding"></div></div></div><button id="multidoc-toggler"><svg viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"></path></svg></button></nav><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="">MCMCDiagnosticTools.jl</a></span></div><form action="search/" class="docs-search"><input class="docs-search-query" id="documenter-search-query" name="q" placeholder="Search docs" type="text"/></form><ul class="docs-menu"><li class="is-active"><a class="tocitem" href="">Home</a><ul class="internal"><li><a class="tocitem" href="#Effective-sample-size-and-\\widehat{R}"><span>Effective sample size and <span>$\widehat{R}$</span></span></a></li><li><a class="tocitem" href="#Monte-Carlo-standard-error"><span>Monte Carlo standard error</span></a></li><li><a class="tocitem" href="#R-diagnostic"><span>R⋆ diagnostic</span></a></li><li><a class="tocitem" href="#Bayesian-fraction-of-missing-information"><span>Bayesian fraction of missing information</span></a></li><li><a class="tocitem" href="#Other-diagnostics"><span>Other diagnostics</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="">Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="">Home</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/main/docs/src/index.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" href="#" id="documenter-settings-button" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" href="#" id="documenter-sidebar-button"></a></div></header><article class="content" id="documenter-page"><h1 id="MCMCDiagnosticTools"><a class="docs-heading-anchor" href="#MCMCDiagnosticTools">MCMCDiagnosticTools</a><a id="MCMCDiagnosticTools-1"></a><a class="docs-heading-anchor-permalink" href="#MCMCDiagnosticTools" title="Permalink"></a></h1><h2 id="Effective-sample-size-and-\\widehat{R}"><a class="docs-heading-anchor" href="#Effective-sample-size-and-\\widehat{R}">Effective sample size and <span>$\widehat{R}$</span></a><a id="Effective-sample-size-and-\\widehat{R}-1"></a><a class="docs-heading-anchor-permalink" href="#Effective-sample-size-and-\\widehat{R}" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.ess" id="MCMCDiagnosticTools.ess"><code>MCMCDiagnosticTools.ess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ess(
    samples::AbstractArray{&lt;:Union{Missing,Real},3};
    kind=:bulk,
    method=ESSMethod(),
    split_chains::Int=2,
    maxlag::Int=250,
    kwargs...
)</code></pre><p>Estimate the effective sample size (ESS) of the <code>samples</code> of shape <code>(draws, chains, parameters)</code> with the <code>method</code>.</p><p>Optionally, the <code>kind</code> of ESS estimate to be computed can be specified (see below). Some <code>kind</code>s accept additional <code>kwargs</code>.</p><p><code>split_chains</code> indicates the number of chains each chain is split into. When <code>split_chains &gt; 1</code>, then the diagnostics check for within-chain convergence. When <code>d = mod(draws, split_chains) &gt; 0</code>, i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first <code>d</code> splits within each chain. There must be at least 3 draws in each chain after splitting.</p><p><code>maxlag</code> indicates the maximum lag for which autocovariance is computed and must be greater than 0.</p><p>For a given estimand, it is recommended that the ESS is at least <code>100 * chains</code> and that <span>$\widehat{R} &lt; 1.01$</span>.<sup class="footnote-reference"><a href="#footnote-VehtariGelman2021" id="citeref-VehtariGelman2021">[VehtariGelman2021]</a></sup></p><p>See also: <a href="#MCMCDiagnosticTools.ESSMethod"><code>ESSMethod</code></a>, <a href="#MCMCDiagnosticTools.FFTESSMethod"><code>FFTESSMethod</code></a>, <a href="#MCMCDiagnosticTools.BDAESSMethod"><code>BDAESSMethod</code></a>, <a href="#MCMCDiagnosticTools.rhat"><code>rhat</code></a>, <a href="#MCMCDiagnosticTools.ess_rhat"><code>ess_rhat</code></a>, <a href="#MCMCDiagnosticTools.mcse"><code>mcse</code></a></p><p><strong>Kinds of ESS estimates</strong></p><p>If <code>kind</code> isa a <code>Symbol</code>, it may take one of the following values:</p><ul><li><code>:bulk</code>: basic ESS computed on rank-normalized draws. This kind diagnoses poor convergence   in the bulk of the distribution due to trends or different locations of the chains.</li><li><code>:tail</code>: minimum of the quantile-ESS for the symmetric quantiles where   <code>tail_prob=0.1</code> is the probability in the tails. This kind diagnoses poor convergence in   the tails of the distribution. If this kind is chosen, <code>kwargs</code> may contain a   <code>tail_prob</code> keyword.</li><li><code>:basic</code>: basic ESS, equivalent to specifying <code>kind=Statistics.mean</code>.</li></ul><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>While Bulk-ESS is conceptually related to basic ESS, it is well-defined even if the chains do not have finite variance.<sup class="footnote-reference"><a href="#footnote-VehtariGelman2021" id="citeref-VehtariGelman2021">[VehtariGelman2021]</a></sup> For each parameter, rank-normalization proceeds by first ranking the inputs using "tied ranking" and then transforming the ranks to normal quantiles so that the result is standard normally distributed. This transform is monotonic.</p></div></div><p>Otherwise, <code>kind</code> specifies one of the following estimators, whose ESS is to be estimated:</p><ul><li><code>Statistics.mean</code></li><li><code>Statistics.median</code></li><li><code>Statistics.std</code></li><li><code>StatsBase.mad</code></li><li><code>Base.Fix2(Statistics.quantile, p::Real)</code></li></ul></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L202-L259" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.rhat" id="MCMCDiagnosticTools.rhat"><code>MCMCDiagnosticTools.rhat</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rhat(samples::AbstractArray{Union{Real,Missing},3}; kind::Symbol=:rank, split_chains=2)</code></pre><p>Compute the <span>$\widehat{R}$</span> diagnostics for each parameter in <code>samples</code> of shape <code>(chains, draws, parameters)</code>.<sup class="footnote-reference"><a href="#footnote-VehtariGelman2021" id="citeref-VehtariGelman2021">[VehtariGelman2021]</a></sup></p><p><code>kind</code> indicates the kind of <span>$\widehat{R}$</span> to compute (see below).</p><p><code>split_chains</code> indicates the number of chains each chain is split into. When <code>split_chains &gt; 1</code>, then the diagnostics check for within-chain convergence. When <code>d = mod(draws, split_chains) &gt; 0</code>, i.e. the chains cannot be evenly split, then 1 draw is discarded after each of the first <code>d</code> splits within each chain.</p><p>See also <a href="#MCMCDiagnosticTools.ess"><code>ess</code></a>, <a href="#MCMCDiagnosticTools.ess_rhat"><code>ess_rhat</code></a>, <a href="#MCMCDiagnosticTools.rstar"><code>rstar</code></a></p><p><strong>Kinds of <span>$\widehat{R}$</span></strong></p><p>The following <code>kind</code>s are supported:</p><ul><li><code>:rank</code>: maximum of <span>$\widehat{R}$</span> with <code>kind=:bulk</code> and <code>kind=:tail</code>.</li><li><code>:bulk</code>: basic <span>$\widehat{R}$</span> computed on rank-normalized draws. This kind diagnoses   poor convergence in the bulk of the distribution due to trends or different locations of   the chains.</li><li><code>:tail</code>: <span>$\widehat{R}$</span> computed on draws folded around the median and then   rank-normalized. This kind diagnoses poor convergence in the tails of the distribution   due to different scales of the chains.</li><li><code>:basic</code>: Classic <span>$\widehat{R}$</span>.</li></ul></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L300-L329" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.ess_rhat" id="MCMCDiagnosticTools.ess_rhat"><code>MCMCDiagnosticTools.ess_rhat</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ess_rhat(samples::AbstractArray{&lt;:Union{Missing,Real},3}; kind::Symbol=:rank, kwargs...)</code></pre><p>Estimate the effective sample size and <span>$\widehat{R}$</span> of the <code>samples</code> of shape <code>(draws, chains, parameters)</code> with the <code>method</code>.</p><p>When both ESS and <span>$\widehat{R}$</span> are needed, this method is often more efficient than calling <code>ess</code> and <code>rhat</code> separately.</p><p>See <a href="#MCMCDiagnosticTools.rhat"><code>rhat</code></a> for a description of supported <code>kind</code>s and <a href="#MCMCDiagnosticTools.ess"><code>ess</code></a> for a description of <code>kwargs</code>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L413-L424" target="_blank">source</a></section></article><p>The following <code>method</code>s are supported:</p><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.ESSMethod" id="MCMCDiagnosticTools.ESSMethod"><code>MCMCDiagnosticTools.ESSMethod</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ESSMethod &lt;: AbstractESSMethod</code></pre><p>The <code>ESSMethod</code> uses a standard algorithm for estimating the effective sample size of MCMC chains.</p><p>It is is based on the discussion by <sup class="footnote-reference"><a href="#footnote-VehtariGelman2021" id="citeref-VehtariGelman2021">[VehtariGelman2021]</a></sup> and uses the biased estimator of the autocovariance, as discussed by <sup class="footnote-reference"><a href="#footnote-Geyer1992" id="citeref-Geyer1992">[Geyer1992]</a></sup>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L9-L24" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.FFTESSMethod" id="MCMCDiagnosticTools.FFTESSMethod"><code>MCMCDiagnosticTools.FFTESSMethod</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">FFTESSMethod &lt;: AbstractESSMethod</code></pre><p>The <code>FFTESSMethod</code> uses a standard algorithm for estimating the effective sample size of MCMC chains.</p><p>The algorithm is the same as the one of <a href="#MCMCDiagnosticTools.ESSMethod"><code>ESSMethod</code></a> but this method uses fast Fourier transforms (FFTs) for estimating the autocorrelation.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>To be able to use this method, you have to load a package that implements the <a href="https://github.com/JuliaMath/AbstractFFTs.jl">AbstractFFTs.jl</a> interface such as <a href="https://github.com/JuliaMath/FFTW.jl">FFTW.jl</a> or <a href="https://github.com/JuliaApproximation/FastTransforms.jl">FastTransforms.jl</a>.</p></div></div></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L27-L41" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.BDAESSMethod" id="MCMCDiagnosticTools.BDAESSMethod"><code>MCMCDiagnosticTools.BDAESSMethod</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">BDAESSMethod &lt;: AbstractESSMethod</code></pre><p>The <code>BDAESSMethod</code> uses a standard algorithm for estimating the effective sample size of MCMC chains.</p><p>It is is based on the discussion by <sup class="footnote-reference"><a href="#footnote-VehtariGelman2021" id="citeref-VehtariGelman2021">[VehtariGelman2021]</a></sup>. and uses the variogram estimator of the autocorrelation function discussed by <sup class="footnote-reference"><a href="#footnote-BDA3" id="citeref-BDA3">[BDA3]</a></sup>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/ess_rhat.jl#L44-L59" target="_blank">source</a></section></article><h2 id="Monte-Carlo-standard-error"><a class="docs-heading-anchor" href="#Monte-Carlo-standard-error">Monte Carlo standard error</a><a id="Monte-Carlo-standard-error-1"></a><a class="docs-heading-anchor-permalink" href="#Monte-Carlo-standard-error" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.mcse" id="MCMCDiagnosticTools.mcse"><code>MCMCDiagnosticTools.mcse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mcse(samples::AbstractArray{&lt;:Union{Missing,Real}}; kind=Statistics.mean, kwargs...)</code></pre><p>Estimate the Monte Carlo standard errors (MCSE) of the estimator <code>kind</code> applied to <code>samples</code> of shape <code>(draws, chains, parameters)</code>.</p><p>See also: <a href="#MCMCDiagnosticTools.ess"><code>ess</code></a></p><p><strong>Kinds of MCSE estimates</strong></p><p>The estimator whose MCSE should be estimated is specified with <code>kind</code>. <code>kind</code> must accept a vector of the same <code>eltype</code> as <code>samples</code> and return a real estimate.</p><p>For the following estimators, the effective sample size <a href="#MCMCDiagnosticTools.ess"><code>ess</code></a> and an estimate of the asymptotic variance are used to compute the MCSE, and <code>kwargs</code> are forwarded to <code>ess</code>:</p><ul><li><code>Statistics.mean</code></li><li><code>Statistics.median</code></li><li><code>Statistics.std</code></li><li><code>Base.Fix2(Statistics.quantile, p::Real)</code></li></ul><p>For other estimators, the subsampling bootstrap method (SBM)<sup class="footnote-reference"><a href="#footnote-FlegalJones2011" id="citeref-FlegalJones2011">[FlegalJones2011]</a></sup><sup class="footnote-reference"><a href="#footnote-Flegal2012" id="citeref-Flegal2012">[Flegal2012]</a></sup> is used as a fallback, and the only accepted <code>kwargs</code> are <code>batch_size</code>, which indicates the size of the overlapping batches used to estimate the MCSE, defaulting to <code>floor(Int, sqrt(draws * chains))</code>. Note that SBM tends to underestimate the MCSE, especially for highly autocorrelated chains. One should verify that autocorrelation is low by checking the bulk- and tail-ESS values.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/mcse.jl#L4-L39" target="_blank">source</a></section></article><h2 id="R-diagnostic"><a class="docs-heading-anchor" href="#R-diagnostic">R⋆ diagnostic</a><a id="R-diagnostic-1"></a><a class="docs-heading-anchor-permalink" href="#R-diagnostic" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.rstar" id="MCMCDiagnosticTools.rstar"><code>MCMCDiagnosticTools.rstar</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rstar(
    rng::Random.AbstractRNG=Random.default_rng(),
    classifier,
    samples,
    chain_indices::AbstractVector{Int};
    subset::Real=0.7,
    split_chains::Int=2,
    verbosity::Int=0,
)</code></pre><p>Compute the <span>$R^*$</span> convergence statistic of the table <code>samples</code> with the <code>classifier</code>.</p><p><code>samples</code> must be either an <code>AbstractMatrix</code>, an <code>AbstractVector</code>, or a table (i.e. implements the Tables.jl interface) whose rows are draws and whose columns are parameters.</p><p><code>chain_indices</code> indicates the chain ids of each row of <code>samples</code>.</p><p>This method supports ragged chains, i.e. chains of nonequal lengths.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/rstar.jl#L1-L21" target="_blank">source</a></section><section><div><pre><code class="nohighlight hljs">rstar(
    rng::Random.AbstractRNG=Random.default_rng(),
    classifier,
    samples::AbstractArray{&lt;:Real,3};
    subset::Real=0.7,
    split_chains::Int=2,
    verbosity::Int=0,
)</code></pre><p>Compute the <span>$R^*$</span> convergence statistic of the <code>samples</code> with the <code>classifier</code>.</p><p><code>samples</code> is an array of draws with the shape <code>(draws, chains, parameters)</code>.`</p><p>This implementation is an adaption of algorithms 1 and 2 described by Lambert and Vehtari.</p><p>The <code>classifier</code> has to be a supervised classifier of the MLJ framework (see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/#model_list">MLJ documentation</a> for a list of supported models). It is trained with a <code>subset</code> of the samples from each chain. Each chain is split into <code>split_chains</code> separate chains to additionally check for within-chain convergence. The training of the classifier can be inspected by adjusting the <code>verbosity</code> level.</p><p>If the classifier is deterministic, i.e., if it predicts a class, the value of the <span>$R^*$</span> statistic is returned (algorithm 1). If the classifier is probabilistic, i.e., if it outputs probabilities of classes, the scaled Poisson-binomial distribution of the <span>$R^*$</span> statistic is returned (algorithm 2).</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The correctness of the statistic depends on the convergence of the <code>classifier</code> used internally in the statistic.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using MLJBase, MLJIteration, EvoTrees, Statistics

julia&gt; samples = fill(4.0, 100, 3, 2);</code></pre><p>One can compute the distribution of the <span>$R^*$</span> statistic (algorithm 2) with a probabilistic classifier. For instance, we can use a gradient-boosted trees model with <code>nrounds = 100</code> sequentially stacked trees and learning rate <code>eta = 0.05</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; model = EvoTreeClassifier(; nrounds=100, eta=0.05);

julia&gt; distribution = rstar(model, samples);

julia&gt; round(mean(distribution); digits=2)
1.0f0</code></pre><p>Note, however, that it is recommended to determine <code>nrounds</code> based on early-stopping. With the MLJ framework, this can be achieved in the following way (see the <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/controlling_iterative_models/">MLJ documentation</a> for additional explanations):</p><pre><code class="language-julia-repl hljs">julia&gt; model = IteratedModel(;
           model=EvoTreeClassifier(; eta=0.05),
           iteration_parameter=:nrounds,
           resampling=Holdout(),
           measures=log_loss,
           controls=[Step(5), Patience(2), NumberLimit(100)],
           retrain=true,
       );

julia&gt; distribution = rstar(model, samples);

julia&gt; round(mean(distribution); digits=2)
1.0f0</code></pre><p>For deterministic classifiers, a single <span>$R^*$</span> statistic (algorithm 1) is returned. Deterministic classifiers can also be derived from probabilistic classifiers by e.g. predicting the mode. In MLJ this corresponds to a pipeline of models.</p><pre><code class="language-julia-repl hljs">julia&gt; evotree_deterministic = Pipeline(model; operation=predict_mode);

julia&gt; value = rstar(evotree_deterministic, samples);

julia&gt; round(value; digits=2)
1.0</code></pre><p><strong>References</strong></p><p>Lambert, B., &amp; Vehtari, A. (2020). <span>$R^*$</span>: A robust MCMC convergence diagnostic with uncertainty using decision tree classifiers.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/rstar.jl#L121-L209" target="_blank">source</a></section></article><h2 id="Bayesian-fraction-of-missing-information"><a class="docs-heading-anchor" href="#Bayesian-fraction-of-missing-information">Bayesian fraction of missing information</a><a id="Bayesian-fraction-of-missing-information-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-fraction-of-missing-information" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.bfmi" id="MCMCDiagnosticTools.bfmi"><code>MCMCDiagnosticTools.bfmi</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bfmi(energy::AbstractVector{&lt;:Real}) -&gt; Real
bfmi(energy::AbstractMatrix{&lt;:Real}; dims::Int=1) -&gt; AbstractVector{&lt;:Real}</code></pre><p>Calculate the estimated Bayesian fraction of missing information (BFMI).</p><p>When sampling with Hamiltonian Monte Carlo (HMC), BFMI quantifies how well momentum resampling matches the marginal energy distribution.</p><p>The current advice is that values smaller than 0.3 indicate poor sampling. However, this threshold is provisional and may change. A BFMI value below the threshold often indicates poor adaptation of sampling parameters or that the target distribution has heavy tails that were not well explored by the Markov chain.</p><p>For more information, see Section 6.1 of <sup class="footnote-reference"><a href="#footnote-Betancourt2018" id="citeref-Betancourt2018">[Betancourt2018]</a></sup> or <sup class="footnote-reference"><a href="#footnote-Betancourt2016" id="citeref-Betancourt2016">[Betancourt2016]</a></sup> for a complete account.</p><p><code>energy</code> is either a vector of Hamiltonian energies of draws or a matrix of energies of draws for multiple chains. <code>dims</code> indicates the dimension in <code>energy</code> that contains the draws. The default <code>dims=1</code> assumes <code>energy</code> has the shape <code>draws</code> or <code>(draws, chains)</code>. If a different shape is provided, <code>dims</code> must be set accordingly.</p><p>If <code>energy</code> is a vector, a single BFMI value is returned. Otherwise, a vector of BFMI values for each chain is returned.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/bfmi.jl#L1-L34" target="_blank">source</a></section></article><h2 id="Other-diagnostics"><a class="docs-heading-anchor" href="#Other-diagnostics">Other diagnostics</a><a id="Other-diagnostics-1"></a><a class="docs-heading-anchor-permalink" href="#Other-diagnostics" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.discretediag" id="MCMCDiagnosticTools.discretediag"><code>MCMCDiagnosticTools.discretediag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">discretediag(samples::AbstractArray{&lt;:Real,3}; frac=0.3, method=:weiss, nsim=1_000)</code></pre><p>Compute discrete diagnostic on <code>samples</code> with shape <code>(draws, chains, parameters)</code>.</p><p><code>method</code> can be one of <code>:weiss</code>, <code>:hangartner</code>, <code>:DARBOOT</code>, <code>:MCBOOT</code>, <code>:billinsgley</code>, and <code>:billingsleyBOOT</code>.</p><p><strong>References</strong></p><p>Benjamin E. Deonovic, &amp; Brian J. Smith. (2017). Convergence diagnostics for MCMC draws of a categorical variable.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/discretediag.jl#L425-L436" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.gelmandiag" id="MCMCDiagnosticTools.gelmandiag"><code>MCMCDiagnosticTools.gelmandiag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gelmandiag(samples::AbstractArray{&lt;:Real,3}; alpha::Real=0.95)</code></pre><p>Compute the Gelman, Rubin and Brooks diagnostics <sup class="footnote-reference"><a href="#footnote-Gelman1992" id="citeref-Gelman1992">[Gelman1992]</a></sup><sup class="footnote-reference"><a href="#footnote-Brooks1998" id="citeref-Brooks1998">[Brooks1998]</a></sup> on <code>samples</code> with shape <code>(draws, chains, parameters)</code>.  Values of the diagnostic’s potential scale reduction factor (PSRF) that are close to one suggest convergence.  As a rule-of-thumb, convergence is rejected if the 97.5 percentile of a PSRF is greater than 1.2.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/gelmandiag.jl#L55-L67" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.gelmandiag_multivariate" id="MCMCDiagnosticTools.gelmandiag_multivariate"><code>MCMCDiagnosticTools.gelmandiag_multivariate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gelmandiag_multivariate(samples::AbstractArray{&lt;:Real,3}; alpha::Real=0.05)</code></pre><p>Compute the multivariate Gelman, Rubin and Brooks diagnostics on <code>samples</code> with shape <code>(draws, chains, parameters)</code>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/gelmandiag.jl#L74-L79" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.gewekediag" id="MCMCDiagnosticTools.gewekediag"><code>MCMCDiagnosticTools.gewekediag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gewekediag(x::AbstractVector{&lt;:Real}; first::Real=0.1, last::Real=0.5, kwargs...)</code></pre><p>Compute the Geweke diagnostic <sup class="footnote-reference"><a href="#footnote-Geweke1991" id="citeref-Geweke1991">[Geweke1991]</a></sup> from the <code>first</code> and <code>last</code> proportion of samples <code>x</code>.</p><p>The diagnostic is designed to asses convergence of posterior means estimated with autocorrelated samples.  It computes a normal-based test statistic comparing the sample means in two windows containing proportions of the first and last iterations.  Users should ensure that there is sufficient separation between the two windows to assume that their samples are independent.  A non-significant test p-value indicates convergence.  Significant p-values indicate non-convergence and the possible need to discard initial samples as a burn-in sequence or to simulate additional samples.</p><p><code>kwargs</code> are forwarded to <a href="#MCMCDiagnosticTools.mcse"><code>mcse</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/gewekediag.jl#L1-L18" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.heideldiag" id="MCMCDiagnosticTools.heideldiag"><code>MCMCDiagnosticTools.heideldiag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">heideldiag(
    x::AbstractVector{&lt;:Real}; alpha::Real=0.05, eps::Real=0.1, start::Int=1, kwargs...
)</code></pre><p>Compute the Heidelberger and Welch diagnostic <sup class="footnote-reference"><a href="#footnote-Heidelberger1983" id="citeref-Heidelberger1983">[Heidelberger1983]</a></sup>. This diagnostic tests for non-convergence (non-stationarity) and whether ratios of estimation interval halfwidths to means are within a target ratio. Stationarity is rejected (0) for significant test p-values. Halfwidth tests are rejected (0) if observed ratios are greater than the target, as is the case for <code>s2</code> and <code>beta[1]</code>.</p><p><code>kwargs</code> are forwarded to <a href="#MCMCDiagnosticTools.mcse"><code>mcse</code></a>.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/heideldiag.jl#L1-L15" target="_blank">source</a></section></article><article class="docstring"><header><a class="docstring-binding" href="#MCMCDiagnosticTools.rafterydiag" id="MCMCDiagnosticTools.rafterydiag"><code>MCMCDiagnosticTools.rafterydiag</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rafterydiag(
    x::AbstractVector{&lt;:Real}; q=0.025, r=0.005, s=0.95, eps=0.001, range=1:length(x)
)</code></pre><p>Compute the Raftery and Lewis diagnostic <sup class="footnote-reference"><a href="#footnote-Raftery1992" id="citeref-Raftery1992">[Raftery1992]</a></sup>. This diagnostic is used to determine the number of iterations required to estimate a specified quantile <code>q</code> within a desired degree of accuracy.  The diagnostic is designed to determine the number of autocorrelated samples required to estimate a specified quantile <span>$\theta_q$</span>, such that <span>$\Pr(\theta \le \theta_q) = q$</span>, within a desired degree of accuracy. In particular, if <span>$\hat{\theta}_q$</span> is the estimand and <span>$\Pr(\theta \le \hat{\theta}_q) = \hat{P}_q$</span> the estimated cumulative probability, then accuracy is specified in terms of <code>r</code> and <code>s</code>, where <span>$\Pr(q - r &lt; \hat{P}_q &lt; q + r) = s$</span>. Thinning may be employed in the calculation of the diagnostic to satisfy its underlying assumptions. However, users may not want to apply the same (or any) thinning when estimating posterior summary statistics because doing so results in a loss of information. Accordingly, sample sizes estimated by the diagnostic tend to be conservative (too large).</p><p>Furthermore, the argument <code>r</code> specifies the margin of error for estimated cumulative probabilities and <code>s</code> the probability for the margin of error. <code>eps</code> specifies the tolerance within which the probabilities of transitioning from initial to retained iterations are within the equilibrium probabilities for the chain. This argument determines the number of samples to discard as a burn-in sequence and is typically left at its default value.</p></div><a class="docs-sourcelink" href="https://github.com/TuringLang/MCMCDiagnosticTools.jl/blob/6088765e3ff7c9c47df54ff5f0aa8a58532406eb/src/rafterydiag.jl#L1-L26" target="_blank">source</a></section></article><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-VehtariGelman2021"><a class="tag is-link" href="#citeref-VehtariGelman2021">VehtariGelman2021</a>Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved <span>$\widehat {R}$</span> for assessing convergence of MCMC. Bayesian Analysis. doi: <a href="https://doi.org/10.1214/20-BA1221">10.1214/20-BA1221</a> arXiv: <a href="https://arxiv.org/abs/1903.08008">1903.08008</a></li><li class="footnote" id="footnote-VehtariGelman2021"><a class="tag is-link" href="#citeref-VehtariGelman2021">VehtariGelman2021</a>Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved <span>$\widehat {R}$</span> for assessing convergence of MCMC. Bayesian Analysis. doi: <a href="https://doi.org/10.1214/20-BA1221">10.1214/20-BA1221</a> arXiv: <a href="https://arxiv.org/abs/1903.08008">1903.08008</a></li><li class="footnote" id="footnote-VehtariGelman2021"><a class="tag is-link" href="#citeref-VehtariGelman2021">VehtariGelman2021</a>Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved <span>$\widehat {R}$</span> for assessing convergence of MCMC. Bayesian Analysis. doi: <a href="https://doi.org/10.1214/20-BA1221">10.1214/20-BA1221</a> arXiv: <a href="https://arxiv.org/abs/1903.08008">1903.08008</a></li><li class="footnote" id="footnote-Geyer1992"><a class="tag is-link" href="#citeref-Geyer1992">Geyer1992</a>Geyer, C. J. (1992). Practical Markov Chain Monte Carlo. Statistical Science, 473-483.</li><li class="footnote" id="footnote-VehtariGelman2021"><a class="tag is-link" href="#citeref-VehtariGelman2021">VehtariGelman2021</a>Vehtari, A., Gelman, A., Simpson, D., Carpenter, B., &amp; Bürkner, P. C. (2021). Rank-normalization, folding, and localization: An improved <span>$\widehat {R}$</span> for assessing convergence of MCMC. Bayesian Analysis. doi: <a href="https://doi.org/10.1214/20-BA1221">10.1214/20-BA1221</a> arXiv: <a href="https://arxiv.org/abs/1903.08008">1903.08008</a></li><li class="footnote" id="footnote-BDA3"><a class="tag is-link" href="#citeref-BDA3">BDA3</a>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. (2013). Bayesian data analysis. CRC press.</li><li class="footnote" id="footnote-FlegalJones2011"><a class="tag is-link" href="#citeref-FlegalJones2011">FlegalJones2011</a>Flegal JM, Jones GL. (2011) Implementing MCMC: estimating with confidence.                 Handbook of Markov Chain Monte Carlo. pp. 175-97.                 <a href="http://faculty.ucr.edu/~jflegal/EstimatingWithConfidence.pdf">pdf</a></li><li class="footnote" id="footnote-Flegal2012"><a class="tag is-link" href="#citeref-Flegal2012">Flegal2012</a>Flegal JM. (2012) Applicability of subsampling bootstrap methods in Markov chain Monte Carlo.            Monte Carlo and Quasi-Monte Carlo Methods 2010. pp. 363-72.            doi: <a href="https://doi.org/10.1007/978-3-642-27440-4_18">10.1007/978-3-642-27440-4_18</a></li><li class="footnote" id="footnote-Betancourt2018"><a class="tag is-link" href="#citeref-Betancourt2018">Betancourt2018</a>Betancourt M. (2018). A Conceptual Introduction to Hamiltonian Monte Carlo. <a href="https://arxiv.org/pdf/1701.02434v2.pdf">arXiv:1701.02434v2</a> [stat.ME]</li><li class="footnote" id="footnote-Betancourt2016"><a class="tag is-link" href="#citeref-Betancourt2016">Betancourt2016</a>Betancourt M. (2016). Diagnosing Suboptimal Cotangent Disintegrations in Hamiltonian Monte Carlo. <a href="https://arxiv.org/pdf/1604.00695v1.pdf">arXiv:1604.00695v1</a> [stat.ME]</li><li class="footnote" id="footnote-Gelman1992"><a class="tag is-link" href="#citeref-Gelman1992">Gelman1992</a>Gelman, A., &amp; Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical science, 7(4), 457-472.</li><li class="footnote" id="footnote-Brooks1998"><a class="tag is-link" href="#citeref-Brooks1998">Brooks1998</a>Brooks, S. P., &amp; Gelman, A. (1998). General methods for monitoring convergence of iterative simulations. Journal of computational and graphical statistics, 7(4), 434-455.</li><li class="footnote" id="footnote-Geweke1991"><a class="tag is-link" href="#citeref-Geweke1991">Geweke1991</a>Geweke, J. F. (1991). Evaluating the accuracy of sampling-based approaches to the calculation of posterior moments (No. 148). Federal Reserve Bank of Minneapolis.</li><li class="footnote" id="footnote-Heidelberger1983"><a class="tag is-link" href="#citeref-Heidelberger1983">Heidelberger1983</a>Heidelberger, P., &amp; Welch, P. D. (1983). Simulation run length control in the presence of an initial transient. Operations Research, 31(6), 1109-1144.</li><li class="footnote" id="footnote-Raftery1992"><a class="tag is-link" href="#citeref-Raftery1992">Raftery1992</a>A L Raftery and S Lewis. Bayesian Statistics, chapter How Many Iterations in the Gibbs Sampler? Volume 4. Oxford University Press, New York, 1992.</li></ul></section></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label></p><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div><p></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 26 February 2023 22:52">Sunday 26 February 2023</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></HTML>